{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.64      0.64      0.64      1477\n",
      "        1.0       0.63      0.64      0.64      1465\n",
      "\n",
      "avg / total       0.64      0.64      0.64      2942\n",
      "\n",
      "[[939 538]\n",
      " [533 932]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\christian.gandon\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# prédire\\nY_pred = classifier.predict(X_test)\\n\\n#confusion matrix\\nfrom sklearn.metrics import confusion_matrix\\ncm = confusion_matrix(Y_test, Y_pred)\\n\\n# CM matrix très mauvais, essayer un kbest pour mieux sélectionner les features\\n\\n#recherche du modele optimum\\nimport statsmodels.formula.api as sm\\n# méthode des moindres carré (ordinary least square)\\nclassifier_logit = sm.Logit(endog = Y, exog = X).fit()\\nclassifier_logit .summary()\\n\\n# nouveau test sans le weight\\nX_opt = X_train[:,[0,1,2,4,5]]\\nclassifier_logit = sm.Logit(endog = Y_train, exog = X_opt).fit()\\nclassifier_logit .summary()\\n\\n# nouveau test sans le weight et sans le reach\\nX_opt = X_train[:,[0,1,4,5]]\\nclassifier_logit = sm.Logit(endog = Y_train, exog = X_opt).fit()\\nclassifier_logit .summary()\\n\\n# nouveau cycle avec modèle optimisé\\n\\nclassifier.fit(X_opt, Y_train)\\n\\nX_test_opt = X_test[:,[0,1,4,5]]\\nY_pred_opt = classifier.predict(X_test_opt)\\n\\n#confusion matrix\\nfrom sklearn.metrics import confusion_matrix\\ncm_opt = confusion_matrix(Y_test, Y_pred_opt)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug 16 20:30:40 2018\n",
    "\n",
    "@author: Christian.GANDON\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue May  8 16:50:48 2018\n",
    "\n",
    "@author: antoinekrainc\n",
    "\"\"\"\n",
    "\n",
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# full execution with logs and graphs?\n",
    "Full = 0\n",
    "\n",
    "\n",
    "# import Dataset \n",
    "dataset = pd.read_csv(\"data\\Boxing_bout_original.csv\", sep=';')\n",
    "\n",
    "# elimination des scores des juges et du type de victoire\n",
    "# elimination des stands car systématiquement == (erreur dataset?)\n",
    "\n",
    "# comment faire des ranges de colonnes pour ne pas les définir une par une??\n",
    "d0 = dataset.iloc[:,[0,1,2,3,4,5,8,9,10,11,12,13,14,15,16,17,18]]\n",
    "d0 = d0.dropna()\n",
    "\n",
    "#encodage variable \"victoire\"\n",
    "\n",
    "d0[\"A_won\"] = 0\n",
    "  \n",
    "for i in d0.index:\n",
    "    if d0.loc[i,\"result\"] == \"win_A\": \n",
    "        d0.loc[i,\"A_won\"] = 1\n",
    "    else:\n",
    "        d0.loc[i,\"A_won\"] = 0\n",
    "\n",
    "#encodage des autres variables\n",
    "d1 = d0.loc[:,\"A_won\"]\n",
    "\n",
    "d1 = pd.DataFrame(d1)\n",
    "\n",
    "d1[\"result\"] = d0[\"result\"]\n",
    "d1[\"age_gap\"] = d0[\"age_A\"] - d0[\"age_B\"]\n",
    "d1[\"height_gap\"] = d0[\"height_A\"] - d0[\"height_B\"]\n",
    "d1[\"reach_gap\"] = d0[\"reach_A\"] - d0[\"reach_B\"]\n",
    "d1[\"weight_gap\"] = d0[\"weight_A\"] - d0[\"weight_B\"]\n",
    "\n",
    "# les criteres historiques sont ramenés a une variable \n",
    "d1[\"win_histo\"] = d0[\"won_A\"] - d0[\"lost_A\"] - d0[\"won_B\"] + d0[\"lost_B\"] \n",
    "\n",
    "#les KO historiques sont estimatés en rapports au différentiel de victoires historiques\n",
    "d1[\"kos_prop\"] = ( d0[\"kos_A\"] / d0[\"won_A\"] ) - ( d0[\"kos_B\"] / d0[\"won_B\"] ) \n",
    "\n",
    "\n",
    "\n",
    "# élimination des valeurs abérrantes sur age and reach gaps\n",
    "for i in d1.index:\n",
    "    if (((d1.loc[i,\"age_gap\"])**2)**(1/2)) > 50:\n",
    "        if Full == 1:\n",
    "            print(\"éliminé: index \", i, d1.loc[i,\"age_gap\"], ((d1.loc[i,\"age_gap\"])**2)**(1/2))\n",
    "        d1.loc[i,\"age_gap\"] = None\n",
    "     \n",
    "    if (((d1.loc[i,\"reach_gap\"])**2)**(1/2)) > 50:\n",
    "        if Full == 1:\n",
    "            print(\"éliminé: index \", i, d1.loc[i,\"reach_gap\"], ((d1.loc[i,\"reach_gap\"])**2)**(1/2))\n",
    "        d1.loc[i,\"reach_gap\"] = None\n",
    "\n",
    "d1 = d1.dropna()\n",
    "\n",
    "\n",
    "# pour éviter la surpondération des \"A_won\" de 80%, j'append le data set une deuxieme fois en inversant tous les paramètres et le résultat\n",
    "\n",
    "\n",
    "dbis = pd.DataFrame()\n",
    "\n",
    "dbis[\"A_won\"] = ((d1[\"A_won\"] - 1)**2)**(1/2)\n",
    "dbis[\"result\"] = d1[\"result\"]\n",
    "dbis[\"age_gap\"] = d1[\"age_gap\"] * -1\n",
    "dbis[\"height_gap\"] = d1[\"height_gap\"] * -1\n",
    "dbis[\"reach_gap\"] = d1[\"reach_gap\"] * -1\n",
    "dbis[\"weight_gap\"] = d1[\"weight_gap\"] * -1\n",
    "dbis[\"win_histo\"] =  d1[\"win_histo\"] * -1\n",
    "dbis[\"kos_prop\"] = d1[\"kos_prop\"] * -1\n",
    "\n",
    "    \n",
    "    \n",
    "d1 = d1.append(dbis)\n",
    "\n",
    "\n",
    "# exploration\n",
    "if Full == 1:\n",
    "    sns.pairplot(d1, hue=\"result\")\n",
    "    \n",
    "    sns.lmplot(x=\"age_gap\", y=\"A_won\", data = d1, logistic = True)\n",
    "    sns.lmplot(x=\"height_gap\", y=\"A_won\", data = d1, logistic = True)\n",
    "    sns.lmplot(x=\"reach_gap\", y=\"A_won\", data = d1, logistic = True)\n",
    "    sns.lmplot(x=\"weight_gap\", y=\"A_won\", data = d1, logistic = True)\n",
    "    sns.lmplot(x=\"win_histo\", y=\"A_won\", data = d1, logistic = True)\n",
    "    sns.lmplot(x=\"kos_prop\", y=\"A_won\", data = d1, logistic = True)\n",
    "# hypothèse \n",
    "\n",
    "X = d1.iloc[:,2:9]\n",
    "Y = d1.loc[:,\"A_won\"]\n",
    "\n",
    "# Divide dataset Train set & Test set \n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 10000)\n",
    "\n",
    "# Feature Scaling <=> normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.fit_transform(X_test)\n",
    "\n",
    "# Application du modèle de classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Decision Tree Classifier\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# make predictions\n",
    "expected = Y_test\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "\"\"\"\n",
    "# prédire\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "# CM matrix très mauvais, essayer un kbest pour mieux sélectionner les features\n",
    "\n",
    "#recherche du modele optimum\n",
    "import statsmodels.formula.api as sm\n",
    "# méthode des moindres carré (ordinary least square)\n",
    "classifier_logit = sm.Logit(endog = Y, exog = X).fit()\n",
    "classifier_logit .summary()\n",
    "\n",
    "# nouveau test sans le weight\n",
    "X_opt = X_train[:,[0,1,2,4,5]]\n",
    "classifier_logit = sm.Logit(endog = Y_train, exog = X_opt).fit()\n",
    "classifier_logit .summary()\n",
    "\n",
    "# nouveau test sans le weight et sans le reach\n",
    "X_opt = X_train[:,[0,1,4,5]]\n",
    "classifier_logit = sm.Logit(endog = Y_train, exog = X_opt).fit()\n",
    "classifier_logit .summary()\n",
    "\n",
    "# nouveau cycle avec modèle optimisé\n",
    "\n",
    "classifier.fit(X_opt, Y_train)\n",
    "\n",
    "X_test_opt = X_test[:,[0,1,4,5]]\n",
    "Y_pred_opt = classifier.predict(X_test_opt)\n",
    "\n",
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_opt = confusion_matrix(Y_test, Y_pred_opt)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-74ed5dcca499>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-74ed5dcca499>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    sns.lmplot(x=\"height_gap\", y=\"A_won\", data = d1, logistic = True)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "sns.lmplot(x=\"age_gap\", y=\"A_won\", data = d1, logistic = True)\n",
    "    sns.lmplot(x=\"height_gap\", y=\"A_won\", data = d1, logistic = True)\n",
    "    sns.lmplot(x=\"reach_gap\", y=\"A_won\", data = d1, logistic = True)\n",
    "    sns.lmplot(x=\"weight_gap\", y=\"A_won\", data = d1, logistic = True)\n",
    "    sns.lmplot(x=\"win_histo\", y=\"A_won\", data = d1, logistic = True)\n",
    "    sns.lmplot(x=\"kos_prop\", y=\"A_won\", data = d1, logistic = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1d639381400>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.lmplot(x=\"age_gap\", y=\"A_won\", data = d1, logistic = True)\n",
    "sns.lmplot(x=\"height_gap\", y=\"A_won\", data = d1, logistic = True)\n",
    "sns.lmplot(x=\"reach_gap\", y=\"A_won\", data = d1, logistic = True)\n",
    "sns.lmplot(x=\"weight_gap\", y=\"A_won\", data = d1, logistic = True)\n",
    "sns.lmplot(x=\"win_histo\", y=\"A_won\", data = d1, logistic = True)\n",
    "sns.lmplot(x=\"kos_prop\", y=\"A_won\", data = d1, logistic = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
